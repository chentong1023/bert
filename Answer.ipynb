{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1OXa6HhY3FYh8o-vK9HLbjvNxCkcK99mi",
      "authorship_tag": "ABX9TyMQo5+bktDgCRjSaPY3wssY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chentong1023/bert/blob/master/Answer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u59inRad8hFM",
        "colab_type": "text"
      },
      "source": [
        "# 关联推理-文本分类大作业（MNLI）\n",
        "\n",
        "姓名：陈彤\n",
        "\n",
        "学号：518030910416\n",
        "\n",
        "模型选择：bert(BERT-BASE)+keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "he85vqgHElqg",
        "colab_type": "text"
      },
      "source": [
        "加载数据"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1Y2F36FdBCO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import csv\n",
        "from bert4keras.backend import keras, set_gelu\n",
        "from bert4keras.tokenizers import Tokenizer\n",
        "from bert4keras.models import build_transformer_model\n",
        "from bert4keras.optimizers import Adam, extend_with_piecewise_linear_lr\n",
        "from bert4keras.snippets import sequence_padding, DataGenerator\n",
        "from bert4keras.snippets import open\n",
        "from keras.layers import Lambda, Dense\n",
        "from keras.utils.np_utils import to_categorical\n",
        "\n",
        "set_gelu('tanh')\n",
        "\n",
        "num_classes = 3\n",
        "maxlen = 128\n",
        "batch_size = 32\n",
        "config_path = './bert_config.json'\n",
        "checkpoint_path = './bert_model.ckpt'\n",
        "dict_path = './vocab.txt'\n",
        "\n",
        "\n",
        "def get_Ids(first_text, second_text):\n",
        "  fir = first_text.lower()\n",
        "  sec = second_text.lower()\n",
        "  return tokenizer.encode(first_text=fir,second_text=sec)\n",
        "\n",
        "def load_train(filename):\n",
        "    D = []\n",
        "    def trans_label(label):\n",
        "      if (label == 'entailment'):\n",
        "        return 0\n",
        "      elif (label == 'neutral'):\n",
        "        return 1\n",
        "      else:\n",
        "        return 2\n",
        "    with open(filename, encoding='utf-8') as f:\n",
        "      rows = csv.DictReader(f)\n",
        "      for row in rows:\n",
        "        D.append(([str(row['premise1']) + str(row['premise2']) + str(row['premise3']) + str(row['premise4']), str(row['hypothesis'])], to_categorical(trans_label(row['label']), num_classes=num_classes) ))\n",
        "    return D\n",
        "\n",
        "def load_test(filename):\n",
        "    D = []\n",
        "    with open(filename, encoding='utf-8') as f:\n",
        "      rows = csv.DictReader(f)\n",
        "      for row in rows:\n",
        "        D.append([str(row['premise1']) + str(row['premise2']) + str(row['premise3']) + str(row['premise4']), str(row['hypothesis'])])\n",
        "    return D\n",
        "\n",
        "# 加载数据集\n",
        "train_data = load_train('./train.csv')\n",
        "test_data = load_test('./test.csv')\n",
        "\n",
        "train_token = []\n",
        "test_token = []\n",
        "\n",
        "# 建立分词器\n",
        "tokenizer = Tokenizer(dict_path, do_lower_case=True)\n",
        "\n",
        "train_str = []\n",
        "train_label = []\n",
        "for text, label in train_data:\n",
        "  token_ids, segment_ids = tokenizer.encode(text[0], text[1], max_length=maxlen)\n",
        "  train_token.append(([token_ids, segment_ids], label))\n",
        "\n",
        "for text in test_data:\n",
        "  token_ids, segment_ids = tokenizer.encode(text[0], text[1], max_length=maxlen)\n",
        "  test_token.append([token_ids, segment_ids])\n",
        "\n",
        "class data_generator(DataGenerator):\n",
        "    # 数据生成器\n",
        "    def __iter__(self, random=False):\n",
        "        batch_token_ids, batch_segment_ids, batch_labels = [], [], []\n",
        "        for is_end, (text, label) in self.sample(random):\n",
        "            token_ids, segment_ids = text[0], text[1]\n",
        "            batch_token_ids.append(token_ids)\n",
        "            batch_segment_ids.append(segment_ids)\n",
        "            batch_labels.append(label)\n",
        "            if len(batch_token_ids) == self.batch_size or is_end:\n",
        "                batch_token_ids = sequence_padding(batch_token_ids)\n",
        "                batch_segment_ids = sequence_padding(batch_segment_ids)\n",
        "                batch_labels = sequence_padding(batch_labels)\n",
        "                yield [batch_token_ids, batch_segment_ids], [batch_labels]\n",
        "                batch_token_ids, batch_segment_ids, batch_labels = [], [], []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dT6f-QebE6On",
        "colab_type": "text"
      },
      "source": [
        "预训练"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TfGiZrfXEsaZ",
        "colab_type": "code",
        "outputId": "7d3dad3d-b01e-4f7f-8bb2-f1198065b68c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# 加载预训练模型\n",
        "bert = build_transformer_model(\n",
        "    config_path=config_path,\n",
        "    checkpoint_path=checkpoint_path,\n",
        "    model='bert',\n",
        "    return_keras_model=False,\n",
        ")\n",
        "\n",
        "output = Lambda(lambda x: x[:, 0], name='CLS-token')(bert.model.output)\n",
        "output = Dense(\n",
        "    units=num_classes,\n",
        "    activation='softmax',\n",
        "    kernel_initializer=bert.initializer\n",
        ")(output)\n",
        "\n",
        "model = keras.models.Model(bert.model.input, output)\n",
        "model.summary()\n",
        "\n",
        "# 派生为带分段线性学习率的优化器。\n",
        "# 其中name参数可选，但最好填入，以区分不同的派生优化器。\n",
        "AdamLR = extend_with_piecewise_linear_lr(Adam, name='Adam')\n",
        "\n",
        "model.compile(\n",
        "    loss='categorical_crossentropy',\n",
        "    optimizer=Adam(5e-8),  # 用足够小的学习率\n",
        "    \n",
        "    #optimizer=AdamLR(learning_rate=1e-4, lr_schedule={\n",
        "    #    1000: 1,\n",
        "    #    2000: 0.1\n",
        "    #}),\n",
        "    metrics=['categorical_accuracy'],\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_12\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "Input-Token (InputLayer)        (None, None)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Input-Segment (InputLayer)      (None, None)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Token (Embedding)     (None, None, 768)    23440896    Input-Token[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Segment (Embedding)   (None, None, 768)    1536        Input-Segment[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Token-Segment (Add)   (None, None, 768)    0           Embedding-Token[0][0]            \n",
            "                                                                 Embedding-Segment[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Position (PositionEmb (None, None, 768)    393216      Embedding-Token-Segment[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Norm (LayerNormalizat (None, None, 768)    1536        Embedding-Position[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Dropout (Dropout)     (None, None, 768)    0           Embedding-Norm[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "Transformer-0-MultiHeadSelfAtte (None, None, 768)    2362368     Embedding-Dropout[0][0]          \n",
            "                                                                 Embedding-Dropout[0][0]          \n",
            "                                                                 Embedding-Dropout[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "Transformer-0-MultiHeadSelfAtte (None, None, 768)    0           Transformer-0-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-0-MultiHeadSelfAtte (None, None, 768)    0           Embedding-Dropout[0][0]          \n",
            "                                                                 Transformer-0-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-0-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-0-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-0-FeedForward (Feed (None, None, 768)    4722432     Transformer-0-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-0-FeedForward-Dropo (None, None, 768)    0           Transformer-0-FeedForward[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Transformer-0-FeedForward-Add ( (None, None, 768)    0           Transformer-0-MultiHeadSelfAttent\n",
            "                                                                 Transformer-0-FeedForward-Dropout\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-0-FeedForward-Norm  (None, None, 768)    1536        Transformer-0-FeedForward-Add[0][\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-1-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-0-FeedForward-Norm[0]\n",
            "                                                                 Transformer-0-FeedForward-Norm[0]\n",
            "                                                                 Transformer-0-FeedForward-Norm[0]\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-1-MultiHeadSelfAtte (None, None, 768)    0           Transformer-1-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-1-MultiHeadSelfAtte (None, None, 768)    0           Transformer-0-FeedForward-Norm[0]\n",
            "                                                                 Transformer-1-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-1-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-1-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-1-FeedForward (Feed (None, None, 768)    4722432     Transformer-1-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-1-FeedForward-Dropo (None, None, 768)    0           Transformer-1-FeedForward[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Transformer-1-FeedForward-Add ( (None, None, 768)    0           Transformer-1-MultiHeadSelfAttent\n",
            "                                                                 Transformer-1-FeedForward-Dropout\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-1-FeedForward-Norm  (None, None, 768)    1536        Transformer-1-FeedForward-Add[0][\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-2-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-1-FeedForward-Norm[0]\n",
            "                                                                 Transformer-1-FeedForward-Norm[0]\n",
            "                                                                 Transformer-1-FeedForward-Norm[0]\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-2-MultiHeadSelfAtte (None, None, 768)    0           Transformer-2-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-2-MultiHeadSelfAtte (None, None, 768)    0           Transformer-1-FeedForward-Norm[0]\n",
            "                                                                 Transformer-2-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-2-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-2-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-2-FeedForward (Feed (None, None, 768)    4722432     Transformer-2-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-2-FeedForward-Dropo (None, None, 768)    0           Transformer-2-FeedForward[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Transformer-2-FeedForward-Add ( (None, None, 768)    0           Transformer-2-MultiHeadSelfAttent\n",
            "                                                                 Transformer-2-FeedForward-Dropout\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-2-FeedForward-Norm  (None, None, 768)    1536        Transformer-2-FeedForward-Add[0][\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-3-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-2-FeedForward-Norm[0]\n",
            "                                                                 Transformer-2-FeedForward-Norm[0]\n",
            "                                                                 Transformer-2-FeedForward-Norm[0]\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-3-MultiHeadSelfAtte (None, None, 768)    0           Transformer-3-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-3-MultiHeadSelfAtte (None, None, 768)    0           Transformer-2-FeedForward-Norm[0]\n",
            "                                                                 Transformer-3-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-3-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-3-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-3-FeedForward (Feed (None, None, 768)    4722432     Transformer-3-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-3-FeedForward-Dropo (None, None, 768)    0           Transformer-3-FeedForward[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Transformer-3-FeedForward-Add ( (None, None, 768)    0           Transformer-3-MultiHeadSelfAttent\n",
            "                                                                 Transformer-3-FeedForward-Dropout\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-3-FeedForward-Norm  (None, None, 768)    1536        Transformer-3-FeedForward-Add[0][\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-4-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-3-FeedForward-Norm[0]\n",
            "                                                                 Transformer-3-FeedForward-Norm[0]\n",
            "                                                                 Transformer-3-FeedForward-Norm[0]\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-4-MultiHeadSelfAtte (None, None, 768)    0           Transformer-4-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-4-MultiHeadSelfAtte (None, None, 768)    0           Transformer-3-FeedForward-Norm[0]\n",
            "                                                                 Transformer-4-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-4-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-4-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-4-FeedForward (Feed (None, None, 768)    4722432     Transformer-4-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-4-FeedForward-Dropo (None, None, 768)    0           Transformer-4-FeedForward[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Transformer-4-FeedForward-Add ( (None, None, 768)    0           Transformer-4-MultiHeadSelfAttent\n",
            "                                                                 Transformer-4-FeedForward-Dropout\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-4-FeedForward-Norm  (None, None, 768)    1536        Transformer-4-FeedForward-Add[0][\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-5-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-4-FeedForward-Norm[0]\n",
            "                                                                 Transformer-4-FeedForward-Norm[0]\n",
            "                                                                 Transformer-4-FeedForward-Norm[0]\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-5-MultiHeadSelfAtte (None, None, 768)    0           Transformer-5-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-5-MultiHeadSelfAtte (None, None, 768)    0           Transformer-4-FeedForward-Norm[0]\n",
            "                                                                 Transformer-5-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-5-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-5-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-5-FeedForward (Feed (None, None, 768)    4722432     Transformer-5-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-5-FeedForward-Dropo (None, None, 768)    0           Transformer-5-FeedForward[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Transformer-5-FeedForward-Add ( (None, None, 768)    0           Transformer-5-MultiHeadSelfAttent\n",
            "                                                                 Transformer-5-FeedForward-Dropout\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-5-FeedForward-Norm  (None, None, 768)    1536        Transformer-5-FeedForward-Add[0][\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-6-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-5-FeedForward-Norm[0]\n",
            "                                                                 Transformer-5-FeedForward-Norm[0]\n",
            "                                                                 Transformer-5-FeedForward-Norm[0]\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-6-MultiHeadSelfAtte (None, None, 768)    0           Transformer-6-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-6-MultiHeadSelfAtte (None, None, 768)    0           Transformer-5-FeedForward-Norm[0]\n",
            "                                                                 Transformer-6-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-6-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-6-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-6-FeedForward (Feed (None, None, 768)    4722432     Transformer-6-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-6-FeedForward-Dropo (None, None, 768)    0           Transformer-6-FeedForward[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Transformer-6-FeedForward-Add ( (None, None, 768)    0           Transformer-6-MultiHeadSelfAttent\n",
            "                                                                 Transformer-6-FeedForward-Dropout\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-6-FeedForward-Norm  (None, None, 768)    1536        Transformer-6-FeedForward-Add[0][\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-7-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-6-FeedForward-Norm[0]\n",
            "                                                                 Transformer-6-FeedForward-Norm[0]\n",
            "                                                                 Transformer-6-FeedForward-Norm[0]\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-7-MultiHeadSelfAtte (None, None, 768)    0           Transformer-7-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-7-MultiHeadSelfAtte (None, None, 768)    0           Transformer-6-FeedForward-Norm[0]\n",
            "                                                                 Transformer-7-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-7-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-7-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-7-FeedForward (Feed (None, None, 768)    4722432     Transformer-7-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-7-FeedForward-Dropo (None, None, 768)    0           Transformer-7-FeedForward[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Transformer-7-FeedForward-Add ( (None, None, 768)    0           Transformer-7-MultiHeadSelfAttent\n",
            "                                                                 Transformer-7-FeedForward-Dropout\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-7-FeedForward-Norm  (None, None, 768)    1536        Transformer-7-FeedForward-Add[0][\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-8-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-7-FeedForward-Norm[0]\n",
            "                                                                 Transformer-7-FeedForward-Norm[0]\n",
            "                                                                 Transformer-7-FeedForward-Norm[0]\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-8-MultiHeadSelfAtte (None, None, 768)    0           Transformer-8-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-8-MultiHeadSelfAtte (None, None, 768)    0           Transformer-7-FeedForward-Norm[0]\n",
            "                                                                 Transformer-8-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-8-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-8-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-8-FeedForward (Feed (None, None, 768)    4722432     Transformer-8-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-8-FeedForward-Dropo (None, None, 768)    0           Transformer-8-FeedForward[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Transformer-8-FeedForward-Add ( (None, None, 768)    0           Transformer-8-MultiHeadSelfAttent\n",
            "                                                                 Transformer-8-FeedForward-Dropout\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-8-FeedForward-Norm  (None, None, 768)    1536        Transformer-8-FeedForward-Add[0][\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-9-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-8-FeedForward-Norm[0]\n",
            "                                                                 Transformer-8-FeedForward-Norm[0]\n",
            "                                                                 Transformer-8-FeedForward-Norm[0]\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-9-MultiHeadSelfAtte (None, None, 768)    0           Transformer-9-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-9-MultiHeadSelfAtte (None, None, 768)    0           Transformer-8-FeedForward-Norm[0]\n",
            "                                                                 Transformer-9-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-9-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-9-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-9-FeedForward (Feed (None, None, 768)    4722432     Transformer-9-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-9-FeedForward-Dropo (None, None, 768)    0           Transformer-9-FeedForward[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Transformer-9-FeedForward-Add ( (None, None, 768)    0           Transformer-9-MultiHeadSelfAttent\n",
            "                                                                 Transformer-9-FeedForward-Dropout\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-9-FeedForward-Norm  (None, None, 768)    1536        Transformer-9-FeedForward-Add[0][\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-10-MultiHeadSelfAtt (None, None, 768)    2362368     Transformer-9-FeedForward-Norm[0]\n",
            "                                                                 Transformer-9-FeedForward-Norm[0]\n",
            "                                                                 Transformer-9-FeedForward-Norm[0]\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-10-MultiHeadSelfAtt (None, None, 768)    0           Transformer-10-MultiHeadSelfAtten\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-10-MultiHeadSelfAtt (None, None, 768)    0           Transformer-9-FeedForward-Norm[0]\n",
            "                                                                 Transformer-10-MultiHeadSelfAtten\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-10-MultiHeadSelfAtt (None, None, 768)    1536        Transformer-10-MultiHeadSelfAtten\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-10-FeedForward (Fee (None, None, 768)    4722432     Transformer-10-MultiHeadSelfAtten\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-10-FeedForward-Drop (None, None, 768)    0           Transformer-10-FeedForward[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Transformer-10-FeedForward-Add  (None, None, 768)    0           Transformer-10-MultiHeadSelfAtten\n",
            "                                                                 Transformer-10-FeedForward-Dropou\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-10-FeedForward-Norm (None, None, 768)    1536        Transformer-10-FeedForward-Add[0]\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-11-MultiHeadSelfAtt (None, None, 768)    2362368     Transformer-10-FeedForward-Norm[0\n",
            "                                                                 Transformer-10-FeedForward-Norm[0\n",
            "                                                                 Transformer-10-FeedForward-Norm[0\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-11-MultiHeadSelfAtt (None, None, 768)    0           Transformer-11-MultiHeadSelfAtten\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-11-MultiHeadSelfAtt (None, None, 768)    0           Transformer-10-FeedForward-Norm[0\n",
            "                                                                 Transformer-11-MultiHeadSelfAtten\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-11-MultiHeadSelfAtt (None, None, 768)    1536        Transformer-11-MultiHeadSelfAtten\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-11-FeedForward (Fee (None, None, 768)    4722432     Transformer-11-MultiHeadSelfAtten\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-11-FeedForward-Drop (None, None, 768)    0           Transformer-11-FeedForward[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Transformer-11-FeedForward-Add  (None, None, 768)    0           Transformer-11-MultiHeadSelfAtten\n",
            "                                                                 Transformer-11-FeedForward-Dropou\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-11-FeedForward-Norm (None, None, 768)    1536        Transformer-11-FeedForward-Add[0]\n",
            "__________________________________________________________________________________________________\n",
            "CLS-token (Lambda)              (None, 768)          0           Transformer-11-FeedForward-Norm[0\n",
            "__________________________________________________________________________________________________\n",
            "dense_438 (Dense)               (None, 3)            2307        CLS-token[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 108,893,955\n",
            "Trainable params: 108,893,955\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cv8CNOtXFId6",
        "colab_type": "text"
      },
      "source": [
        "训练和评估"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZHm3fzbEjAx",
        "colab_type": "code",
        "outputId": "9fd6f95f-e5b0-4796-fa76-91885d4939b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 737
        }
      },
      "source": [
        "def evaluate(data):\n",
        "    total, right = 0., 0.\n",
        "    for x_true, y_true in data:\n",
        "        token_ids, segment_ids = get_Ids(x_test[0],x_test[1])\n",
        "        y_pred = model.predict([np.array([token_ids]),np.array([segment_ids])]).argmax(axis=1)\n",
        "        y_indx = np.array(y_true).argmax()\n",
        "        total += 1\n",
        "        right += (y_indx == y_pred)\n",
        "    return right / total\n",
        "\n",
        "\n",
        "class Evaluator(keras.callbacks.Callback):\n",
        "    def __init__(self):\n",
        "        self.best_val_acc = 0.\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "      val_acc = evaluate(valid_generator)\n",
        "      if val_acc > self.best_val_acc:\n",
        "          self.best_val_acc = val_acc\n",
        "          model.save_weights('best_model.weights')\n",
        "      print(\n",
        "          u'val_acc: %.5f, best_val_acc: %.5f\\n' %\n",
        "          (val_acc, self.best_val_acc)\n",
        "      )\n",
        "\n",
        "train_generator = data_generator(train_token, batch_size)\n",
        "valid_generator = data_generator(train_token, batch_size)\n",
        "evaluator = Evaluator()\n",
        "model.load_weights(\"best_model.weights\")\n",
        "model.fit_generator(\n",
        "    train_generator.forfit(),\n",
        "    steps_per_epoch=len(train_generator),\n",
        "    epochs=10\n",
        "    ,callbacks=[evaluator]\n",
        ")\n",
        "model.load_weights(\"best_model.weights\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "282/282 [==============================] - 336s 1s/step - loss: 0.1745 - categorical_accuracy: 0.9447\n",
            "val_acc: 0.37943, best_val_acc: 0.37943\n",
            "\n",
            "Epoch 2/10\n",
            "282/282 [==============================] - 337s 1s/step - loss: 0.1685 - categorical_accuracy: 0.9474\n",
            "val_acc: 0.37943, best_val_acc: 0.37943\n",
            "\n",
            "Epoch 3/10\n",
            "282/282 [==============================] - 336s 1s/step - loss: 0.1701 - categorical_accuracy: 0.9460\n",
            "val_acc: 0.37943, best_val_acc: 0.37943\n",
            "\n",
            "Epoch 4/10\n",
            "282/282 [==============================] - 337s 1s/step - loss: 0.1674 - categorical_accuracy: 0.9473\n",
            "val_acc: 0.37943, best_val_acc: 0.37943\n",
            "\n",
            "Epoch 5/10\n",
            "282/282 [==============================] - 336s 1s/step - loss: 0.1629 - categorical_accuracy: 0.9474\n",
            "val_acc: 0.37943, best_val_acc: 0.37943\n",
            "\n",
            "Epoch 6/10\n",
            "282/282 [==============================] - 335s 1s/step - loss: 0.1605 - categorical_accuracy: 0.9490\n",
            "val_acc: 0.37943, best_val_acc: 0.37943\n",
            "\n",
            "Epoch 7/10\n",
            "282/282 [==============================] - 336s 1s/step - loss: 0.1597 - categorical_accuracy: 0.9498\n",
            "val_acc: 0.37943, best_val_acc: 0.37943\n",
            "\n",
            "Epoch 8/10\n",
            "282/282 [==============================] - 337s 1s/step - loss: 0.1643 - categorical_accuracy: 0.9489\n",
            "val_acc: 0.37943, best_val_acc: 0.37943\n",
            "\n",
            "Epoch 9/10\n",
            "282/282 [==============================] - 337s 1s/step - loss: 0.1568 - categorical_accuracy: 0.9511\n",
            "val_acc: 0.37943, best_val_acc: 0.37943\n",
            "\n",
            "Epoch 10/10\n",
            "282/282 [==============================] - 336s 1s/step - loss: 0.1548 - categorical_accuracy: 0.9517\n",
            "val_acc: 0.37943, best_val_acc: 0.37943\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfA2h7qMFTt5",
        "colab_type": "text"
      },
      "source": [
        "输出预测"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fb42lncFSze",
        "colab_type": "code",
        "outputId": "2a7198f0-1761-40bb-d728-8e48347cfd36",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 793
        }
      },
      "source": [
        "# 转换数据集\n",
        "\n",
        "label_names = [\"entailment\",\"neutral\",\"contradiction\"]\n",
        "pred_label=[]\n",
        "cnter = 0\n",
        "for x_test in test_data:\n",
        "  if cnter <= 20:\n",
        "    print (x_test)\n",
        "  token_ids, segment_ids = get_Ids(x_test[0],x_test[1])\n",
        "  y_pred = model.predict([np.array([token_ids]),np.array([segment_ids])]).argmax(axis=1)\n",
        "  pred_label.append(label_names[y_pred[0]])\n",
        "  if cnter <= 20:\n",
        "    print(y_pred)\n",
        "    cnter = cnter+1\n",
        "with open(\"predict.csv\", \"w\") as fo:\n",
        "  writer = csv.writer(fo)\n",
        "  i=0\n",
        "  writer.writerow([\"ID\", \"label\"])\n",
        "  for lb in pred_label:\n",
        "    writer.writerow([i, lb])\n",
        "    i=i+1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Three male safety workers in neon yellow vests are taking their break.Three workers in bright green vests, taking a break.Three construction workers sit down and take a break.Construction workers have a hard job.', 'Men sit on a bench.']\n",
            "[1]\n",
            "['Several young adults sit in a room with a projector on the ceiling.Students sitting in a classroom are learning about art.A bunch of kids in a classroom not doing any work.A meeting in a room with six people.', 'A group sitting around a table.']\n",
            "[0]\n",
            "['Young women and men on a stairwell talking with pictures of the United States behind them.Woman in tie-dye shirt and another woman sing on the steps.Young adults sitting on the stairs talking.Young people sitting on steps.', 'A group sits.']\n",
            "[0]\n",
            "['Children stand with guns on a large green lawn with a man dressed as a soldier.A man dressed as a British Guard entertains children holding toy guns.Kids are watching a man in uniform standing on the grass.A soldier is teaching kids how to handle a gun.', 'Children learning.']\n",
            "[0]\n",
            "['Two young girls are playing on swings, and one is holding the other girls swing \" still. \"Two teenage girls take turns helping each other on the swings in the park.Two girls are swinging in harnessed rides.Two women are swinging on a ride.', 'Women sitting.']\n",
            "[1]\n",
            "['Five men dressed as women in black fur coats and glasses, outside of a store on the street, all holding purses in their hands.Five people are dressed in black fur coats with blond wigs leaning on a building.Men dressed in drag outside a store.Five men in drag holding purses', 'Women stand and talk.']\n",
            "[1]\n",
            "['A young girl around ten years old hitting what appears to be a tennis ball with a golf club from a tee.A golfer plays golf in the middle of a golf course which contains assorted colors of cones.A sportswoman in a red, white, and blue shirt finishes with a swing.A young lady holding a golf club above her head.', 'Some women standing.']\n",
            "[2]\n",
            "['A man is standing at the front of a conference room with people sitting at rows of tables next to a table with a laptop and projector.A man stands beside a projector in a dimly lit conference room while some people who are wearing dress shirts are talking.A man is standing by a table in a banquet room filled with people and tables.A dining room full of people eating and a waiter serving them.', 'A large group watches a presentation.']\n",
            "[0]\n",
            "['A crowd of people are soaking something, one girl is dressed up in a sports bra and red bottoms and one guy is dressed up in camouflage.A Caucasian woman listens as an Asian man in camouflage explains something.A group of people on the beach looking at their green buckets.A group of oriental people are gathering clams on the beach.', 'Group sitting.']\n",
            "[0]\n",
            "['Seven people sitting around a black table playing a card game.A gathering of people sit around a table and play cards.A group of people at dining table playing a card game.Several young people sit at a table playing poker.', 'Men and women playing a game.']\n",
            "[0]\n",
            "['Three people walking down a snowy path, wearing winter gear.Three men, crosscountry skiing, in a field of snow.Three individuals climbing up a mountain on skis.Three people skiing behind one another in snow.', 'A skier jumping.']\n",
            "[1]\n",
            "['A skier is jumping in the air over a snowfield near a mountain range.A skier performs a high jump in a snow-covered valley.A snow skier is taking a huge leap over a snow slope.Skier in red jumping high in the air over the snow.', 'A skier coming down the mountain.']\n",
            "[0]\n",
            "['Older man with a striped shirt is waiting for someone to buy his flowers.A man in a pink striped shirt is selling flowers out of baskets.A man in a striped pink shirt cells flower petals.An Asian gentlemen is selling flowers at market.', 'A smiling Asian woman sitting.']\n",
            "[2]\n",
            "['An older man in a purple shirt sitting on a bench with his foot on a pole.An elderly man is sitting on a bench with his foot propped up on a tree.A man sits on a part bench with his foot up against a tree.People sitting next to trees on a bench.', 'A group of older women sitting.']\n",
            "[2]\n",
            "['Three construction workers wearing orange working vests and white helmets performing work in a tunnel.Four city workers wearing orange vests are working on platforms in a subway.Construction workers are working at a train or subway platform.Men wearing orange vests are working on the subway.', 'A man bicycling.']\n",
            "[2]\n",
            "['While doing tricks, the skateboarder is at the top of the half pipe.A teenage male grinds on his skateboard.A person in a red shirt on a skateboard.A person riding a skateboard.', 'A skater does a trick.']\n",
            "[0]\n",
            "['Dark-skinned female dressed in white and black martial arts uniform strikes a lunge pose.Demonstration of martial arts by young female in gymnasium.A black belt practices her martial arts barefoot.Young women practicing martial arts in a gym.', 'A person chases a calf inside a rodeo ring.']\n",
            "[2]\n",
            "['With majestic pride the horse soars over the miniature barn as the rider holds on.A person on a large gray horse jumping over a miniature house with a red roof.A white horse jumping over a hurdle that looks like a small barn.A woman and a horse are jumping over a miniature building.', 'A rider riding an animal on a track.']\n",
            "[0]\n",
            "['A gray dog jumps through the air over a gravel path.The furry brown dog is jumping on the concrete.A dog leaping on pavement.A black dog jumping.', 'A dog runs across a street.']\n",
            "[2]\n",
            "['Young men and women sit at a wooden table on which several partially-drunk beverages have been placed; the picture is interrupted by an outstretched hand, colored green in the odd light of the wood-paneled interior.A green tinted hand is being held up in front of a table of people.Two guys gather around a table with two girls while drinking.People socializing at a table with beverages.', 'Friends hanging out.']\n",
            "[0]\n",
            "['Young male white boy who is dressed in blue jeans and yellow long-sleeved shirt is climbing a tree.A small boy wearing a yellow sweatshirt is climbing up a fruit tree.Young boy in an orange shirt climbing an apple tree.A child in a yellow shirt is climbing a tree.', 'Some individuals sitting.']\n",
            "[2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ND4zj0tLFrYe",
        "colab_type": "text"
      },
      "source": [
        "以下为环境的配置："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zUYA3RrIVGGz",
        "colab_type": "code",
        "outputId": "35eb8d58-ae1a-4d26-d5db-6dab141a9653",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        }
      },
      "source": [
        "!pip install git+https://www.github.com/bojone/bert4keras.git"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://www.github.com/bojone/bert4keras.git\n",
            "  Cloning https://www.github.com/bojone/bert4keras.git to /tmp/pip-req-build-boyio5_a\n",
            "  Running command git clone -q https://www.github.com/bojone/bert4keras.git /tmp/pip-req-build-boyio5_a\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (from bert4keras==0.7.7) (2.3.1)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras->bert4keras==0.7.7) (1.18.4)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras->bert4keras==0.7.7) (1.12.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras->bert4keras==0.7.7) (1.0.8)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras->bert4keras==0.7.7) (1.1.2)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras->bert4keras==0.7.7) (1.4.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras->bert4keras==0.7.7) (2.10.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras->bert4keras==0.7.7) (3.13)\n",
            "Building wheels for collected packages: bert4keras\n",
            "  Building wheel for bert4keras (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bert4keras: filename=bert4keras-0.7.7-cp36-none-any.whl size=40962 sha256=9b8eb88f9007556d485bae87b1687d891f9e0c4efe5b3d734f2c0de3e7f315d9\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-f7i1z701/wheels/12/58/83/8ff5c864b80c860e6d9e9e0d90c04fafca05d01d21f9f6fcba\n",
            "Successfully built bert4keras\n",
            "Installing collected packages: bert4keras\n",
            "Successfully installed bert4keras-0.7.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uomrAOSUUA09",
        "colab_type": "code",
        "outputId": "a100318c-2065-453c-be56-ffb2678936fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        }
      },
      "source": [
        "!wget https://storage.googleapis.com/bert_models/2020_02_20/uncased_L-12_H-768_A-12.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-05-20 06:50:41--  https://storage.googleapis.com/bert_models/2020_02_20/uncased_L-12_H-768_A-12.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 64.233.188.128, 2404:6800:4008:c07::80\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|64.233.188.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 408102251 (389M) [application/zip]\n",
            "Saving to: ‘uncased_L-12_H-768_A-12.zip’\n",
            "\n",
            "uncased_L-12_H-768_ 100%[===================>] 389.20M   133MB/s    in 2.9s    \n",
            "\n",
            "2020-05-20 06:50:44 (133 MB/s) - ‘uncased_L-12_H-768_A-12.zip’ saved [408102251/408102251]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "muLBiX4CTzRm",
        "colab_type": "code",
        "outputId": "551b9c11-e415-4918-b9ee-c53d1cbee6b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        }
      },
      "source": [
        "!pip install bert4keras"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: bert4keras in /usr/local/lib/python3.6/dist-packages (0.7.7)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (from bert4keras) (2.3.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras->bert4keras) (3.13)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras->bert4keras) (1.1.2)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras->bert4keras) (1.18.4)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras->bert4keras) (1.12.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras->bert4keras) (1.4.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras->bert4keras) (1.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras->bert4keras) (2.10.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6gneL3jUdv3",
        "colab_type": "code",
        "outputId": "7abb5b5e-ecc7-49bc-eb14-15e8c8f837ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "!unzip uncased_L-12_H-768_A-12.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  uncased_L-12_H-768_A-12.zip\n",
            "  inflating: bert_model.ckpt.data-00000-of-00001  \n",
            "  inflating: bert_config.json        \n",
            "  inflating: vocab.txt               \n",
            "  inflating: bert_model.ckpt.index   \n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}